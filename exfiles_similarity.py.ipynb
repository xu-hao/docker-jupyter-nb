{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import livy, pprint, requests\n",
    "host = livy.getHostUrl(\"http://34.205.23.216:9191\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................."
     ]
    }
   ],
   "source": [
    "session_url = livy.openSession(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_url = session_url + \"/upload-file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(upload_file_url, files={\"file\": open(\"gtex_rnaseq_prep_profiles.tsv\",\"rb\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml.linalg import Vectors, Matrices\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "path = SparkFiles.get(\"gtex_rnaseq_prep_profiles.tsv\")\n",
    "\n",
    "dataheaderindex = 2\n",
    "\n",
    "with open(path) as inp:\n",
    "    headers = inp.readline().strip().split(\"\\t\")\n",
    "    # ids = []\n",
    "    # features = []\n",
    "    ncols = len(headers) - dataheaderindex\n",
    "    nrows = 0\n",
    "    data = []\n",
    "    for line in inp:\n",
    "        row = line.strip().split('\\t')\n",
    "        if len(row) < dataheaderindex:\n",
    "            continue\n",
    "        # id = row[:dataheaderindex]\n",
    "        feature = map(float, row[dataheaderindex:])\n",
    "        data.extend(feature)\n",
    "        data2.append(feature)\n",
    "        nrows += 1\n",
    "        \n",
    "df = Matrices.dense(nrows, ncols, data)\n",
    "\n",
    "data2 = map(lambda x : [Vectors.dense(*x)], data2)\n",
    "rdd2 = sc.parallelize(data2)\n",
    "\n",
    "def dot_self(v):\n",
    "    return v.dot(v)\n",
    "    \n",
    "AAl = rdd2.map(dot_self).collect()\n",
    "\n",
    "#ABdf = df.multiply(df2)\n",
    "\n",
    "#def tanimoto(i):\n",
    "#    data = []\n",
    "#    for j in range(i,ncols):\n",
    "#        AB = ABm[i,j]\n",
    "#        data.append(AB / (AAl[i] + AAl[j] - AB))\n",
    "#    return data\n",
    "  \n",
    "#tanimotodf = sc.parallelize([i for i in range(nrows)]).map(tanimoto)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    }
   ],
   "source": [
    "r = livy.execStatement(host, session_url, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': '\\n'\n",
      "         'from pyspark import SparkContext\\n'\n",
      "         'from pyspark import SparkFiles\\n'\n",
      "         'from pyspark.ml.linalg import Vectors, Matrices\\n'\n",
      "         'from pyspark.ml.stat import Correlation\\n'\n",
      "         '\\n'\n",
      "         'path = SparkFiles.get(\"gtex_rnaseq_prep_profiles.tsv\")\\n'\n",
      "         '\\n'\n",
      "         'dataheaderindex = 2\\n'\n",
      "         '\\n'\n",
      "         'with open(path) as inp:\\n'\n",
      "         '    headers = inp.readline().strip().split(\"\\t\")\\n'\n",
      "         '    # ids = []\\n'\n",
      "         '    # features = []\\n'\n",
      "         '    ncols = len(headers) - dataheaderindex\\n'\n",
      "         '    nrows = 0\\n'\n",
      "         '    data = []\\n'\n",
      "         '    for line in inp:\\n'\n",
      "         \"        row = line.strip().split('\\t')\\n\"\n",
      "         '        if len(row) < dataheaderindex:\\n'\n",
      "         '            continue\\n'\n",
      "         '        # id = row[:dataheaderindex]\\n'\n",
      "         '        feature = map(float, row[dataheaderindex:])\\n'\n",
      "         '        data.extend(feature)\\n'\n",
      "         '        data2.append(feature)\\n'\n",
      "         '        nrows += 1\\n'\n",
      "         '        \\n'\n",
      "         'df = Matrices.dense(nrows, ncols, data)\\n'\n",
      "         '\\n'\n",
      "         'data2 = map(lambda x : [Vectors.dense(*x)], data2)\\n'\n",
      "         'rdd2 = sc.parallelize(data2)\\n'\n",
      "         '\\n'\n",
      "         'def dot_self(v):\\n'\n",
      "         '    return v.dot(v)\\n'\n",
      "         '    \\n'\n",
      "         'AAl = rdd2.map(dot_self).collect()\\n'\n",
      "         '\\n'\n",
      "         '#ABdf = df.multiply(df2)\\n'\n",
      "         '\\n'\n",
      "         '#def tanimoto(i):\\n'\n",
      "         '#    data = []\\n'\n",
      "         '#    for j in range(i,ncols):\\n'\n",
      "         '#        AB = ABm[i,j]\\n'\n",
      "         '#        data.append(AB / (AAl[i] + AAl[j] - AB))\\n'\n",
      "         '#    return data\\n'\n",
      "         '  \\n'\n",
      "         '#tanimotodf = sc.parallelize([i for i in '\n",
      "         'range(nrows)]).map(tanimoto)\\n',\n",
      " 'id': 14,\n",
      " 'output': {'ename': 'Py4JJavaError',\n",
      "            'evalue': 'An error occurred while calling '\n",
      "                      'z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n'\n",
      "                      ': org.apache.spark.SparkException: Job aborted due to '\n",
      "                      'stage failure: Task 2 in stage 16.0 failed 4 times, '\n",
      "                      'most recent failure: Lost task 2.3 in stage 16.0 (TID '\n",
      "                      '63, 9.0.1.130, executor 0): '\n",
      "                      'org.apache.spark.api.python.PythonException: Traceback '\n",
      "                      '(most recent call last):\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", '\n",
      "                      'line 229, in main\\n'\n",
      "                      '    process()\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", '\n",
      "                      'line 224, in process\\n'\n",
      "                      '    serializer.dump_stream(func(split_index, iterator), '\n",
      "                      'outfile)\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                      'line 372, in dump_stream\\n'\n",
      "                      '    vs = list(itertools.islice(iterator, batch))\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                      'line 145, in load_stream\\n'\n",
      "                      '    yield self._read_with_length(stream)\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                      'line 170, in _read_with_length\\n'\n",
      "                      '    return self.loads(obj)\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                      'line 559, in loads\\n'\n",
      "                      '    return pickle.loads(obj, encoding=encoding)\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", '\n",
      "                      'line 22, in <module>\\n'\n",
      "                      '    from pyspark.ml.base import Estimator, Model, '\n",
      "                      'Transformer, UnaryTransformer\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", '\n",
      "                      'line 24, in <module>\\n'\n",
      "                      '    from pyspark.ml.param.shared import *\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", '\n",
      "                      'line 26, in <module>\\n'\n",
      "                      '    import numpy as np\\n'\n",
      "                      \"ImportError: No module named 'numpy'\\n\"\n",
      "                      '\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.Iterator$class.foreach(Iterator.scala:893)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.Task.run(Task.scala:109)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\\n'\n",
      "                      '\\tat '\n",
      "                      'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n'\n",
      "                      '\\tat '\n",
      "                      'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n'\n",
      "                      '\\tat java.lang.Thread.run(Thread.java:748)\\n'\n",
      "                      '\\n'\n",
      "                      'Driver stacktrace:\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\\n'\n",
      "                      '\\tat scala.Option.foreach(Option.scala:257)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\\n'\n",
      "                      '\\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\\n'\n",
      "                      '\\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\\n'\n",
      "                      '\\tat '\n",
      "                      'sun.reflect.NativeMethodAccessorImpl.invoke0(Native '\n",
      "                      'Method)\\n'\n",
      "                      '\\tat '\n",
      "                      'sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n'\n",
      "                      '\\tat '\n",
      "                      'sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n'\n",
      "                      '\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n'\n",
      "                      '\\tat '\n",
      "                      'py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n'\n",
      "                      '\\tat '\n",
      "                      'py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n'\n",
      "                      '\\tat py4j.Gateway.invoke(Gateway.java:282)\\n'\n",
      "                      '\\tat '\n",
      "                      'py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n'\n",
      "                      '\\tat '\n",
      "                      'py4j.commands.CallCommand.execute(CallCommand.java:79)\\n'\n",
      "                      '\\tat '\n",
      "                      'py4j.GatewayConnection.run(GatewayConnection.java:214)\\n'\n",
      "                      '\\tat java.lang.Thread.run(Thread.java:748)\\n'\n",
      "                      'Caused by: org.apache.spark.api.python.PythonException: '\n",
      "                      'Traceback (most recent call last):\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", '\n",
      "                      'line 229, in main\\n'\n",
      "                      '    process()\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", '\n",
      "                      'line 224, in process\\n'\n",
      "                      '    serializer.dump_stream(func(split_index, iterator), '\n",
      "                      'outfile)\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                      'line 372, in dump_stream\\n'\n",
      "                      '    vs = list(itertools.islice(iterator, batch))\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                      'line 145, in load_stream\\n'\n",
      "                      '    yield self._read_with_length(stream)\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                      'line 170, in _read_with_length\\n'\n",
      "                      '    return self.loads(obj)\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                      'line 559, in loads\\n'\n",
      "                      '    return pickle.loads(obj, encoding=encoding)\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", '\n",
      "                      'line 22, in <module>\\n'\n",
      "                      '    from pyspark.ml.base import Estimator, Model, '\n",
      "                      'Transformer, UnaryTransformer\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", '\n",
      "                      'line 24, in <module>\\n'\n",
      "                      '    from pyspark.ml.param.shared import *\\n'\n",
      "                      '  File '\n",
      "                      '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", '\n",
      "                      'line 26, in <module>\\n'\n",
      "                      '    import numpy as np\\n'\n",
      "                      \"ImportError: No module named 'numpy'\\n\"\n",
      "                      '\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.Iterator$class.foreach(Iterator.scala:893)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\\n'\n",
      "                      '\\tat '\n",
      "                      'scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.scheduler.Task.run(Task.scala:109)\\n'\n",
      "                      '\\tat '\n",
      "                      'org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\\n'\n",
      "                      '\\tat '\n",
      "                      'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n'\n",
      "                      '\\tat '\n",
      "                      'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n'\n",
      "                      '\\t... 1 more\\n',\n",
      "            'execution_count': 14,\n",
      "            'status': 'error',\n",
      "            'traceback': ['Traceback (most recent call last):\\n',\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", '\n",
      "                          'line 824, in collect\\n'\n",
      "                          '    port = '\n",
      "                          'self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\\n',\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\", '\n",
      "                          'line 1160, in __call__\\n'\n",
      "                          '    answer, self.gateway_client, self.target_id, '\n",
      "                          'self.name)\\n',\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", '\n",
      "                          'line 63, in deco\\n'\n",
      "                          '    return f(*a, **kw)\\n',\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\", '\n",
      "                          'line 320, in get_return_value\\n'\n",
      "                          '    format(target_id, \".\", name), value)\\n',\n",
      "                          'py4j.protocol.Py4JJavaError: An error occurred '\n",
      "                          'while calling '\n",
      "                          'z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n'\n",
      "                          ': org.apache.spark.SparkException: Job aborted due '\n",
      "                          'to stage failure: Task 2 in stage 16.0 failed 4 '\n",
      "                          'times, most recent failure: Lost task 2.3 in stage '\n",
      "                          '16.0 (TID 63, 9.0.1.130, executor 0): '\n",
      "                          'org.apache.spark.api.python.PythonException: '\n",
      "                          'Traceback (most recent call last):\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", '\n",
      "                          'line 229, in main\\n'\n",
      "                          '    process()\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", '\n",
      "                          'line 224, in process\\n'\n",
      "                          '    serializer.dump_stream(func(split_index, '\n",
      "                          'iterator), outfile)\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                          'line 372, in dump_stream\\n'\n",
      "                          '    vs = list(itertools.islice(iterator, batch))\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                          'line 145, in load_stream\\n'\n",
      "                          '    yield self._read_with_length(stream)\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                          'line 170, in _read_with_length\\n'\n",
      "                          '    return self.loads(obj)\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                          'line 559, in loads\\n'\n",
      "                          '    return pickle.loads(obj, encoding=encoding)\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", '\n",
      "                          'line 22, in <module>\\n'\n",
      "                          '    from pyspark.ml.base import Estimator, Model, '\n",
      "                          'Transformer, UnaryTransformer\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", '\n",
      "                          'line 24, in <module>\\n'\n",
      "                          '    from pyspark.ml.param.shared import *\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", '\n",
      "                          'line 26, in <module>\\n'\n",
      "                          '    import numpy as np\\n'\n",
      "                          \"ImportError: No module named 'numpy'\\n\"\n",
      "                          '\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.Iterator$class.foreach(Iterator.scala:893)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.Task.run(Task.scala:109)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\\n'\n",
      "                          '\\tat '\n",
      "                          'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n'\n",
      "                          '\\tat '\n",
      "                          'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n'\n",
      "                          '\\tat java.lang.Thread.run(Thread.java:748)\\n'\n",
      "                          '\\n'\n",
      "                          'Driver stacktrace:\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\\n'\n",
      "                          '\\tat scala.Option.foreach(Option.scala:257)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.rdd.RDD.collect(RDD.scala:938)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\\n'\n",
      "                          '\\tat '\n",
      "                          'sun.reflect.NativeMethodAccessorImpl.invoke0(Native '\n",
      "                          'Method)\\n'\n",
      "                          '\\tat '\n",
      "                          'sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n'\n",
      "                          '\\tat '\n",
      "                          'sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n'\n",
      "                          '\\tat '\n",
      "                          'java.lang.reflect.Method.invoke(Method.java:498)\\n'\n",
      "                          '\\tat '\n",
      "                          'py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n'\n",
      "                          '\\tat '\n",
      "                          'py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n'\n",
      "                          '\\tat py4j.Gateway.invoke(Gateway.java:282)\\n'\n",
      "                          '\\tat '\n",
      "                          'py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n'\n",
      "                          '\\tat '\n",
      "                          'py4j.commands.CallCommand.execute(CallCommand.java:79)\\n'\n",
      "                          '\\tat '\n",
      "                          'py4j.GatewayConnection.run(GatewayConnection.java:214)\\n'\n",
      "                          '\\tat java.lang.Thread.run(Thread.java:748)\\n'\n",
      "                          'Caused by: '\n",
      "                          'org.apache.spark.api.python.PythonException: '\n",
      "                          'Traceback (most recent call last):\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", '\n",
      "                          'line 229, in main\\n'\n",
      "                          '    process()\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", '\n",
      "                          'line 224, in process\\n'\n",
      "                          '    serializer.dump_stream(func(split_index, '\n",
      "                          'iterator), outfile)\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                          'line 372, in dump_stream\\n'\n",
      "                          '    vs = list(itertools.islice(iterator, batch))\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                          'line 145, in load_stream\\n'\n",
      "                          '    yield self._read_with_length(stream)\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                          'line 170, in _read_with_length\\n'\n",
      "                          '    return self.loads(obj)\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", '\n",
      "                          'line 559, in loads\\n'\n",
      "                          '    return pickle.loads(obj, encoding=encoding)\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", '\n",
      "                          'line 22, in <module>\\n'\n",
      "                          '    from pyspark.ml.base import Estimator, Model, '\n",
      "                          'Transformer, UnaryTransformer\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", '\n",
      "                          'line 24, in <module>\\n'\n",
      "                          '    from pyspark.ml.param.shared import *\\n'\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", '\n",
      "                          'line 26, in <module>\\n'\n",
      "                          '    import numpy as np\\n'\n",
      "                          \"ImportError: No module named 'numpy'\\n\"\n",
      "                          '\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.Iterator$class.foreach(Iterator.scala:893)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\\n'\n",
      "                          '\\tat '\n",
      "                          'scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.scheduler.Task.run(Task.scala:109)\\n'\n",
      "                          '\\tat '\n",
      "                          'org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\\n'\n",
      "                          '\\tat '\n",
      "                          'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n'\n",
      "                          '\\tat '\n",
      "                          'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n'\n",
      "                          '\\t... 1 more\\n'\n",
      "                          '\\n']},\n",
      " 'progress': 1.0,\n",
      " 'state': 'available'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text/plain': 'Pearson correlation matrix:\\n'\n",
      "               'Row(pearson(features)=DenseMatrix(9, 9, [1.0, 0.2447, 0.2068, '\n",
      "               '0.4914, 0.3069, -0.0747, 0.1278, 0.3957, ..., 0.0188, 0.2582, '\n",
      "               '0.5914, 0.3462, 0.1979, 0.0447, 0.3077, 1.0], False))\\n'\n",
      "               'Spearman correlation matrix:\\n'\n",
      "               'Row(spearman(features)=DenseMatrix(9, 9, [1.0, 0.223, 0.3006, '\n",
      "               '0.4801, 0.3286, 0.3539, 0.37, 0.5528, ..., 0.1393, 0.2525, '\n",
      "               '0.4988, 0.3717, 0.4857, 0.1324, 0.3615, 1.0], False))'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(r.json()[\"output\"][\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "path = SparkFiles.get(\"gtex_rnaseq_prep_profiles.tsv\")\n",
    "\n",
    "dataheaderindex = 2\n",
    "\n",
    "with open(path) as inp:\n",
    "    headers = inp.readline().strip().split(\"\\t\")\n",
    "    # ids = []\n",
    "    # features = []\n",
    "    data = []\n",
    "    for line in inp:\n",
    "        row = line.strip().split('\\t')\n",
    "        if len(row) < dataheaderindex:\n",
    "            continue\n",
    "        # id = row[:dataheaderindex]\n",
    "        feature = map(float, row[dataheaderindex:])\n",
    "        data.append(feature)\n",
    "    data2 = list(map(lambda x : [Vectors.dense(*x)], zip(*data)))\n",
    "\n",
    "df = spark.createDataFrame(data2, [\"features\"])\n",
    "\n",
    "#r1 = Correlation.corr(df, \"features\").head()\n",
    "#print(\"Pearson correlation matrix:\\\\n\" + str(r1))\n",
    "\n",
    "#r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "#print(\"Spearman correlation matrix:\\\\n\" + str(r2))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = livy.execStatement(host, session_url, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': '\\n'\n",
      "         'from pyspark import SparkContext\\n'\n",
      "         'from pyspark import SparkFiles\\n'\n",
      "         'from pyspark.ml.linalg import Vectors\\n'\n",
      "         'from pyspark.ml.stat import Correlation\\n'\n",
      "         '\\n'\n",
      "         'path = SparkFiles.get(\"gtex_rnaseq_prep_profiles.tsv\")\\n'\n",
      "         '\\n'\n",
      "         'dataheaderindex = 2\\n'\n",
      "         '\\n'\n",
      "         'with open(path) as inp:\\n'\n",
      "         '    headers = inp.readline().strip().split(\"\\t\")\\n'\n",
      "         '    # ids = []\\n'\n",
      "         '    # features = []\\n'\n",
      "         '    data = []\\n'\n",
      "         '    for line in inp:\\n'\n",
      "         \"        row = line.strip().split('\\t')\\n\"\n",
      "         '        if len(row) < dataheaderindex:\\n'\n",
      "         '            continue\\n'\n",
      "         '        # id = row[:dataheaderindex]\\n'\n",
      "         '        feature = map(float, row[dataheaderindex:])\\n'\n",
      "         '        data.append(feature)\\n'\n",
      "         '    data2 = list(map(lambda x : [Vectors.dense(*x)], data))\\n'\n",
      "         '\\n'\n",
      "         'df = spark.createDataFrame(data2, [\"features\"])\\n'\n",
      "         '\\n'\n",
      "         '#r1 = Correlation.corr(df, \"features\").head()\\n'\n",
      "         '#print(\"Pearson correlation matrix:\\\\n\" + str(r1))\\n'\n",
      "         '\\n'\n",
      "         '#r2 = Correlation.corr(df, \"features\", \"spearman\").head()\\n'\n",
      "         '#print(\"Spearman correlation matrix:\\\\n\" + str(r2))\\n',\n",
      " 'id': 10,\n",
      " 'output': {'data': {'text/plain': ''}, 'execution_count': 10, 'status': 'ok'},\n",
      " 'progress': 1.0,\n",
      " 'state': 'available'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text/plain': 'Pearson correlation matrix:\\n'\n",
      "               'Row(pearson(features)=DenseMatrix(9, 9, [1.0, 0.2447, 0.2068, '\n",
      "               '0.4914, 0.3069, -0.0747, 0.1278, 0.3957, ..., 0.0188, 0.2582, '\n",
      "               '0.5914, 0.3462, 0.1979, 0.0447, 0.3077, 1.0], False))\\n'\n",
      "               'Spearman correlation matrix:\\n'\n",
      "               'Row(spearman(features)=DenseMatrix(9, 9, [1.0, 0.223, 0.3006, '\n",
      "               '0.4801, 0.3286, 0.3539, 0.37, 0.5528, ..., 0.1393, 0.2525, '\n",
      "               '0.4988, 0.3717, 0.4857, 0.1324, 0.3615, 1.0], False))'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(r.json()[\"output\"][\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "livy.closeSession(session_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
